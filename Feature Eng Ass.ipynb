{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8a46fe-7aaa-4435-97ec-a4bc2dd8e7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. What is a parameter?\n",
    "\n",
    "A parameter is a configuration variable that the learning algorithm estimates from data during training.\n",
    "Examples: weights in linear regression, coefficients in logistic regression, or the neurons’ weights in a neural network.\n",
    "\n",
    "2. What is correlation? What does negative correlation mean?\n",
    "\n",
    "Correlation measures the strength and direction of a linear relationship between two variables, usually expressed by the correlation coefficient (r) ranging from –1 to +1.\n",
    "It means that when one variable increases, the other tends to decrease.\n",
    "Example: Hours of exercise vs. body fat percentage.\n",
    "\n",
    "3. Define Machine Learning and its main components.\n",
    "\n",
    "Machine Learning is the science of enabling computers to learn patterns from data without explicit programming.\n",
    "Main components:\n",
    "\n",
    "Data (training & testing sets)\n",
    "\n",
    "Features (input variables)\n",
    "\n",
    "Model/Algorithm (e.g., decision tree, neural net)\n",
    "\n",
    "Loss function\n",
    "\n",
    "Optimizer (to minimize loss)\n",
    "\n",
    "Evaluation (metrics such as accuracy, RMSE).\n",
    "\n",
    "4. How does loss value indicate model quality?\n",
    "\n",
    "Lower loss means the model’s predictions are closer to actual outcomes. A consistently high loss signals poor fit.\n",
    "\n",
    "5. What are continuous and categorical variables?\n",
    "\n",
    "Continuous: Numeric values with infinite possible points within a range (e.g., temperature, weight).\n",
    "\n",
    "Categorical: Discrete groups or labels (e.g., gender, city).\n",
    "\n",
    "6. How do we handle categorical variables in Machine Learning? What are the common techniques?\n",
    "\n",
    "Categorical variables represent labels or categories (e.g., colors, countries, types). Most ML algorithms need numerical input, so we convert categories into numbers.\n",
    "Common techniques:\n",
    "\n",
    "Label Encoding\n",
    "\n",
    "One-Hot Encoding\n",
    "\n",
    "Ordinal Encoding (when order matters)\n",
    "\n",
    "Target/Mean Encoding (advanced).\n",
    "\n",
    "7. What do you mean by training and testing a dataset?\n",
    "\n",
    "1. Training a Dataset\n",
    "\n",
    "Purpose: To teach the machine learning model using historical or sample data.\n",
    "\n",
    "What Happens: The model learns patterns and relationships between input features and the target/output.\n",
    "\n",
    "Data Used: Called the training set — usually 70–80% of the total dataset.\n",
    "\n",
    "Example: You train a model to predict house prices using data like size, location, and number of rooms.\n",
    "\n",
    "2. Testing a Dataset\n",
    "\n",
    "Purpose: To evaluate the model’s performance on unseen data.\n",
    "\n",
    "What Happens: The model uses what it learned during training to make predictions, and you compare those predictions to actual outcomes.\n",
    "\n",
    "Data Used: Called the test set — usually 20–30% of the total dataset.\n",
    "\n",
    "Example: After training the house price model, you test it on new houses to see how well it predicts their prices.\n",
    "\n",
    "\n",
    "8. What is sklearn.preprocessing?\n",
    "\n",
    "sklearn.preprocessing is a module in Scikit-learn (a popular Python machine learning library) that provides tools to prepare and transform data before feeding it into machine learning models.\n",
    "\n",
    "9. What is a Test set?\n",
    "A reserved portion of the dataset, typically 20–30%, that remains untouched during training.\n",
    "It provides an unbiased final evaluation of the model’s performance.\n",
    "\n",
    "10. How do we split data for model fitting (training and testing) in Python?\n",
    "How do you approach a Machine Learning problem?\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "test_size=0.2 means 20% of data is for testing.\n",
    "\n",
    "stratify=y preserves class distribution in classification tasks.\n",
    "\n",
    "--> Approach to a Machine Learning Problem\n",
    "\n",
    "Problem Understanding – Define objective and metrics.\n",
    "\n",
    "Data Collection – Gather structured/unstructured data.\n",
    "\n",
    "Data Cleaning – Handle missing values, outliers.\n",
    "\n",
    "Exploratory Data Analysis (EDA) – Visualize distributions, detect correlations.\n",
    "\n",
    "Feature Engineering – Transform and create informative features.\n",
    "\n",
    "Model Selection & Training – Choose algorithms and tune hyperparameters.\n",
    "\n",
    "Evaluation – Use cross-validation, confusion matrix, etc.\n",
    "\n",
    "Deployment & Monitoring – Serve predictions, monitor drift and performance.\n",
    "\n",
    "11. Why do we have to perform EDA before fitting a model to the data?\n",
    "\n",
    "Performing Exploratory Data Analysis (EDA) before fitting a model is a critical step in any machine learning workflow. \n",
    "Here's a clear, point-wise explanation of why EDA is necessary:\n",
    "\n",
    "1. Understand the Data\n",
    "\n",
    "Helps you understand the structure, types, and distribution of data.\n",
    "\n",
    "Example: Are the features numerical or categorical? Are there outliers?\n",
    "\n",
    "2. Detect Missing or Corrupted Data\n",
    "\n",
    "Identifies missing values, NaNs, or data entry errors.\n",
    "\n",
    "You can decide whether to fill, drop, or impute such values before modeling.\n",
    "\n",
    "3. Uncover Patterns and Relationships\n",
    "\n",
    "Helps identify correlations and dependencies between variables.\n",
    "\n",
    "Example: A strong correlation between a feature and the target is useful.\n",
    "\n",
    "4. Identify Outliers and Noise\n",
    "\n",
    "Outliers can distort model predictions and affect accuracy.\n",
    "\n",
    "EDA helps you decide whether to remove, transform, or leave them.\n",
    "\n",
    "5. Feature Selection & Engineering\n",
    "\n",
    "Reveals which features may be useful, redundant, or irrelevant.\n",
    "\n",
    "Helps with creating new, more meaningful features.\n",
    "\n",
    "6. Choose the Right Algorithms\n",
    "\n",
    "Data insights help you choose the best model type.\n",
    "\n",
    "Example: If the target is imbalanced → Use special classifiers or resampling.\n",
    "\n",
    "7. Avoid Wrong Assumptions\n",
    "\n",
    "Prevents blindly applying algorithms to unsuitable data (e.g., non-numeric or skewed distributions).\n",
    "\n",
    "Reduces chances of garbage-in, garbage-out results.     \n",
    "\n",
    "\n",
    "12. What is correlation?        \n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "corr = df.corr()\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm')\n",
    "plt.show()\n",
    "\n",
    "df.corr() computes the Pearson correlation matrix.\n",
    "\n",
    "Heatmaps visualize strength and direction.  \n",
    "\n",
    "13. What does negative correlation mean?\n",
    "\n",
    "A negative correlation means that as one variable increases, the other variable decreases, and vice versa.\n",
    "\n",
    " Direction of Relationship\n",
    "\n",
    "Inverse relationship\n",
    "\n",
    "When X goes up, Y goes down\n",
    "\n",
    "When X goes down, Y goes up\n",
    "\n",
    " Correlation Coefficient (r) Range\n",
    "\n",
    "Value of r is between -1 and 0\n",
    "\n",
    "Closer to -1 → Stronger negative correlation\n",
    "\n",
    "r = -1 → Perfect negative correlation\n",
    "\n",
    "r = 0 → No correlation\n",
    "\n",
    "14. How can you find correlation between variables in Python?\n",
    "\n",
    "Using pandas.corr()\n",
    "import pandas as pd\n",
    "\n",
    "# Load or create a DataFrame\n",
    "df = pd.read_csv(\"your_data.csv\")\n",
    "\n",
    "# Compute correlation matrix\n",
    "correlation_matrix = df.corr()\n",
    "\n",
    "print(correlation_matrix)\n",
    "\n",
    "By default, it uses Pearson correlation.\n",
    "\n",
    "You can also specify other methods:\n",
    "\n",
    "df.corr(method='pearson')\n",
    "\n",
    "df.corr(method='kendall')\n",
    "\n",
    "df.corr(method='spearman')\n",
    "\n",
    "15. What is causation? Explain difference between correlation and causation with an example.\n",
    "\n",
    "Correlation: Two variables move together.\n",
    "\n",
    "Causation: One variable directly influences the other.\n",
    "\n",
    "Example:\n",
    "Hot weather increases ice-cream sales and swimming, leading to more drowning incidents.\n",
    "Ice-cream sales correlate with drownings but do not cause them.\n",
    "\n",
    "To establish causation, controlled experiments or causal inference methods are needed.\n",
    "\n",
    "16. What is an Optimizer? What are different types of optimizers? Explain each with an example.\n",
    "\n",
    "An optimizer updates model parameters to minimize the loss.\n",
    "\n",
    "Common Optimizers\n",
    "\n",
    "Gradient Descent (GD): Updates weights using the entire dataset each step.\n",
    "\n",
    "Stochastic Gradient Descent (SGD): Updates using a single sample—faster, adds noise to escape local minima.\n",
    "\n",
    "Mini-batch GD: Balance of GD and SGD.\n",
    "\n",
    "Adam: Adaptive learning rates; most popular for deep learning.\n",
    "\n",
    "RMSprop & Adagrad: Adjust learning rates per parameter.\n",
    "\n",
    "Eg: \n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "17. What is sklearn.linear_model ?\n",
    "\n",
    "sklearn.linear_model is a module in Scikit-learn that contains linear models for both regression and classification tasks.\n",
    "\n",
    "It provides tools to model relationships between input variables (features) and the target variable (output) using linear equations.\n",
    "\n",
    "18. What does model.fit() do? What arguments must be given?\n",
    "\n",
    "The .fit() method is used to train a machine learning model on your dataset.\n",
    "It learns the patterns in the data by finding the best model parameters.\n",
    "\n",
    "It builds the internal structure of the model (e.g., coefficients in linear regression, tree splits in decision trees).\n",
    "\n",
    "This is the first step before you can use .predict() or .score().\n",
    "\n",
    "model.fit(X, y)\n",
    "\n",
    "Eg:\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Sample data\n",
    "X = [[1], [2], [3], [4]]   # Features\n",
    "y = [2, 4, 6, 8]           # Target\n",
    "\n",
    "# Create model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y)\n",
    "\n",
    "19. What does model.predict() do? What arguments must be given?\n",
    "\n",
    "The .predict() method is used to make predictions using a trained machine learning model.\n",
    "Uses the learned patterns from .fit() to predict the target/output for new or unseen data.\n",
    "\n",
    "Outputs either:\n",
    "\n",
    "Numerical values (for regression)\n",
    "\n",
    "Class labels (for classification)\n",
    "\n",
    "Eg:\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Training data\n",
    "X = [[1], [2], [3], [4]]\n",
    "y = [2, 4, 6, 8]\n",
    "\n",
    "# Train model\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predict for new input\n",
    "X_new = [[5], [6]]\n",
    "predictions = model.predict(X_new)\n",
    "\n",
    "print(predictions)  # Output: [10. 12.]\n",
    "\n",
    "\n",
    "20. What are continuous and categorical variables?\n",
    "\n",
    "1. Continuous Variables\n",
    "\n",
    "Definition: Variables that can take any numeric value within a range (including decimals).\n",
    "\n",
    "Also called: Quantitative or numerical variables\n",
    "\n",
    "Can be measured, not just counted\n",
    "\n",
    " Examples:\n",
    "\n",
    "Height (e.g., 172.5 cm)\n",
    "\n",
    "Temperature (e.g., 36.6°C)\n",
    "\n",
    "Income (e.g., $45,678.90)\n",
    "\n",
    "Time (e.g., 2.75 hours)\n",
    "\n",
    "   2. Categorical Variables\n",
    "\n",
    "Definition: Variables that represent categories or groups. Values are labels and not numerical in a meaningful way.\n",
    "\n",
    "Also called: Qualitative variables\n",
    "\n",
    "Can be counted but not measured \n",
    "\n",
    "\n",
    "21. What is feature scaling? How does it help in Machine Learning?\n",
    "\n",
    "Feature scaling is the process of standardizing or normalizing the range of independent variables (features) in your dataset.\n",
    "Different features can have different units and scales (e.g., age in years vs. income in thousands).\n",
    "\n",
    "Algorithms that compute distances or assume normally distributed data can be biased or perform poorly if features are on different scales.\n",
    "\n",
    "\n",
    "22. How do we perform scaling in Python?\n",
    "\n",
    "Common Scaling Techniques in Python\n",
    "1. Standardization (StandardScaler)\n",
    "\n",
    "Scales features to have mean = 0 and standard deviation = 1\n",
    "Eg: \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Example data: 2D array (samples x features)\n",
    "X = [[1, 50], [2, 60], [3, 70]]\n",
    "\n",
    "# Fit scaler on data and transform it\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(X_scaled)\n",
    "\n",
    "2. Normalization (MinMaxScaler)\n",
    "\n",
    "Scales features to a fixed range (default 0 to 1)\n",
    "\n",
    "eg:\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X = [[1, 50], [2, 60], [3, 70]]\n",
    "\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(X_scaled)\n",
    "\n",
    "\n",
    "23. What is sklearn.preprocessing?\n",
    "\n",
    "sklearn.preprocessing is a module in Scikit-learn that provides tools to prepare and transform your data before feeding it into machine learning models. It includes functions and classes for tasks like:\n",
    "\n",
    "Scaling features (e.g., StandardScaler, MinMaxScaler)\n",
    "\n",
    "Encoding categorical variables (e.g., OneHotEncoder, LabelEncoder)\n",
    "\n",
    "Normalizing data (e.g., Normalizer)\n",
    "\n",
    "Generating polynomial features (e.g., PolynomialFeatures)\n",
    "\n",
    "These preprocessing steps help improve model performance and ensure the data is in the right format for algorithms.\n",
    "\n",
    "24.  How do we split data for model fitting (training and testing) in Python?\n",
    "\n",
    " Splitting data into training and testing sets is a crucial step to evaluate your machine \n",
    " learning model’s performance on unseen data. Here’s how to do it in Python using Scikit-learn:                        \n",
    "\n",
    "Using train_test_split from sklearn.model_selection\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Suppose X = features, y = target variable\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "25. Explain data encoding?\n",
    "\n",
    " Data encoding is the process of transforming categorical variables (non-numeric data) \n",
    " into a numerical format that machine learning models can understand and use.\n",
    "\n",
    "  Most ML algorithms require numerical input.\n",
    "\n",
    "Models can’t work directly with text labels like \"Red\", \"Blue\", or \"Small\".\n",
    "\n",
    "Encoding converts categories into numbers while preserving information.   \n",
    "\n",
    "\n",
    "     Eg:\n",
    "     from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "# Label Encoding example\n",
    "le = LabelEncoder()\n",
    "labels = ['Red', 'Blue', 'Green']\n",
    "encoded_labels = le.fit_transform(labels)  # Output: [2, 0, 1]\n",
    "\n",
    "# One-Hot Encoding example (using pandas for convenience)\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'Color': ['Red', 'Blue', 'Green']})\n",
    "one_hot_encoded = pd.get_dummies(df['Color'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
